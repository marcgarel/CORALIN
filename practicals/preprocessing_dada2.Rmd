---
title: "Preprocessing : dada2"
date: "September 2025"
output: html_document
author:
  - "Fabrice Armougom, MIO"
  - "Marc Garel, MIO"
  - "Nicolas Henry, ABiMS"
  - "Charlotte Berthelier, ABiMS"
editor_options: 
  chunk_output_type: inline
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warnings = FALSE)
```

## Prepare your working directory

### Get the repository files

To begin with, download the course repository on your computer or virtual machine.

To do so, visit the ([ANF-metabarcoding](https://github.com/ANF-MetaBioDiv/course-material){target="_blank"}) repository on github and download it as a zip file.

You can also download the repository directly from the terminal using this commmand:

``` bash
wget https://github.com/ANF-MetaBioDiv/course-material/archive/refs/heads/main.zip
```

Once on your machine, unzip the file and go to the resulting folder or place the resulting folder in the most convenient location (`setwd("~/course-material-main")` for example).

::: tip
ðŸ’¡ **Tip:** Work within an **RStudio project**. Open your course-material-main folder as an RStudio project (create or use a `.Rproj` file inside it). Then here::here("data", "refdb") will correctly point to `/home/rstudio/Documents/course-material-main/data/refdb`.
:::

### Download the reference database

Save as a variable the path to the folder where you will place the references databases.

```{r}
refdb_folder <- here::here("data", "refdb")
refdb_folder
```



```{r}
if (!dir.exists(refdb_folder)) dir.create(refdb_folder, recursive = TRUE)
```


```{r}
# R stop downloading after timeout which is
# 60 seconds by default
getOption("timeout")

# so we change timeout to be 20 minutes
options(timeout = 1200)

# we save in variable the path to the refdb
# in the working space
silva_train_set <- 
  file.path(
    refdb_folder,
    "silva_nr99_v138.2_toGenus_trainset.fa.gz"
  )

silva_species_assignment <- 
  file.path(
    refdb_folder,
    "silva_v138.2_assignSpecies.fa.gz"
  )

# then we download the files if they don't already exist

if (!file.exists(silva_train_set)) {
  download.file(
    "https://zenodo.org/records/14169026/files/silva_nr99_v138.2_toGenus_trainset.fa.gz",
    silva_train_set,
    quiet = TRUE
  )
}

if (!file.exists(silva_species_assignment)) {
  download.file(
    "https://zenodo.org/records/14169026/files/silva_v138.2_assignSpecies.fa.gz",
    silva_species_assignment,
    quiet = TRUE
  )
}

```

### Attach custom functions

```{r}
devtools::load_all()
```

## Inputs files

### Locate the sequencing files

```{r}
path_to_fastqs <- here::here("data", "raw")
```


```{r}
fnFs <- sort(list.files(path_to_fastqs,
                        pattern = "_R1.fastq.gz",
                        full.names = TRUE))
```


```{r}
fnRs <- sort(list.files(path_to_fastqs,
                        pattern = "_R2.fastq.gz",
                        full.names = TRUE))
```



### Extract sample names

```{r}
sample_names <- basename(fnFs) |>
  strsplit(split = "_") |>
  sapply(head, 1)
sample_names 
```

## Sequence quality check
```{r results=FALSE}
# create a directory for the outputs
quality_folder <- here::here("outputs",
                             "dada2",
                             "quality_plots")

if (!dir.exists(quality_folder)) {
  dir.create(quality_folder, recursive = TRUE)
}

qualityprofile(fnFs,
               fnRs,
               file.path(quality_folder, "quality_plots.pdf"))
```


## Trimming and quality filtering

#### Prepare outputs

Same as before, create a folder

```{r}
path_to_filtered_reads <- here::here("outputs", "dada2", "filtered")
if (!dir.exists(path_to_filtered_reads)) dir.create(path_to_filtered_reads, recursive = TRUE)
```


```{r}
filtFs <- file.path(path_to_filtered_reads, basename(fnFs))
filtRs <- file.path(path_to_filtered_reads, basename(fnRs))
```


```{r}
names(filtFs) <- sample_names
names(filtRs) <- sample_names
```

```{r}
(out <- dada2::filterAndTrim(
  fwd = fnFs,
  filt = filtFs,
  rev = fnRs,
  filt.rev = filtRs,
  trimLeft = c(19, 20),
  trimRight = c(0, 40),
  minLen = 150,
  matchIDs = TRUE,
  maxN = 0,
  maxEE = c(2, 3),
  truncQ = 2
))
out
```


## Denoising

### Learn the error model

```{r}
errF <- dada2::learnErrors(filtFs,
                           randomize = TRUE,
                           multithread = TRUE)

errR <- dada2::learnErrors(filtRs,
                           randomize = TRUE,
                           multithread = TRUE)

```

```{r}
dada2::plotErrors(errF, nominalQ = TRUE)
dada2::plotErrors(errR, nominalQ = TRUE)
```

### Dereplication
```{r}
derepFs <- dada2::derepFastq(filtFs, verbose = TRUE)

derepRs <- dada2::derepFastq(filtRs, verbose = TRUE)
```

### Run dada

```{r}
dadaFs <- dada2::dada(derepFs, err = errF, multithread = TRUE)

dadaRs <- dada2::dada(derepRs, err = errR, multithread = TRUE)
```

## Merge paired-end reads

Once forward and reverse reads have been denoised, we can merge them with `dada2::mergePairs()`.

```{r}
mergers <- dada2::mergePairs(
  dadaF = dadaFs,
  derepF = derepFs,
  dadaR = dadaRs,
  derepR = derepRs,
  maxMismatch = 0,
  verbose = TRUE
)
```

## Build the ASV table
```{r}
seqtab <- dada2::makeSequenceTable(mergers)
```

## Remove chimeras
```{r}
seqtab_nochim <- dada2::removeBimeraDenovo(seqtab,
                                           method = "consensus",
                                           multithread = TRUE,
                                           verbose = TRUE)
```

## Taxonomic assignment from dada2

```{r}
taxonomy <- dada2::assignTaxonomy(
  seqs = seqtab_nochim,
  refFasta = silva_train_set,
  taxLevels = c(
    "Kingdom",
    "Phylum",
    "Class",
    "Order",
    "Family",
    "Genus",
    "Species"
  ),
  multithread = TRUE,
  minBoot = 60
)
```

```{r}
taxonomy <- dada2::addSpecies(
  taxonomy,
  silva_species_assignment,
  allowMultiple = FALSE
)
```


## Export

### R objects


```{r}
export_folder <- here::here("outputs", "dada2", "asv_table")

if (!dir.exists(export_folder)) dir.create(export_folder, recursive = TRUE)

saveRDS(object = seqtab_nochim,
        file = file.path(export_folder, "seqtab_nochim.rds"))

saveRDS(object = taxonomy,
        file = file.path(export_folder, "taxonomy.rds"))
```

### Text files


```{r}
asv_seq <- colnames(seqtab_nochim)
```

```{r}
ndigits <- nchar(length(asv_seq))
asv_id <- sprintf(paste0("ASV_%0", ndigits, "d"), seq_along(asv_seq))
```

```{r}
row.names(taxonomy) <- colnames(seqtab_nochim) <- names(asv_seq) <- asv_id
```

```{r}
taxonomy_export <- df_export(taxonomy, new_rn = "asv")

seqtab_nochim_export <- t(seqtab_nochim)
seqtab_nochim_export <- df_export(seqtab_nochim_export, new_rn = "asv")
```

```{r}
write.table(taxonomy_export,
            file = file.path(export_folder, "taxonomy.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

```{r}
write.table(seqtab_nochim_export,
            file = file.path(export_folder, "asv_table.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

```{r}
cat(paste0(">", names(asv_seq), "\n", asv_seq),
    sep = "\n",
    file = file.path(export_folder, "asv.fasta"))
```

### Log

```{r}
getN <- function(x) sum(dada2::getUniques(x))

log_table <- data.frame(
  input = out[, 1],
  filtered = out[, 2],
  denoisedF = sapply(dadaFs, getN),
  denoisedR = sapply(dadaRs, getN),
  merged = sapply(mergers, getN),
  nonchim = rowSums(seqtab_nochim),
  perc_retained = rowSums(seqtab_nochim) / out[, 1] * 100
)

rownames(log_table) <- sample_names
log_table
```

```{r}
df_export(log_table, new_rn = "sample") |>
  write.table(file = file.path(export_folder, "log_table.tsv"),
              quote = FALSE,
              sep = "\t",
              row.names = FALSE)
```


```{r}
library(phyloseq)
```


```{r}
input_dir <- here::here("outputs", "dada2", "asv_table")
```


```{r}
asv_table <- read.table(file = file.path(input_dir, "asv_table.tsv"),
                        header = TRUE,
                        sep = "\t",
                        row.names = 1)
```

the results of the taxonomic assignment:

```{r}
taxonomy <- read.table(file = file.path(input_dir, "taxonomy.tsv"),
                        header = TRUE,
                        sep = "\t",
                        row.names = 1)
```

and the ASV sequences:

```{r}
asv_seq <- Biostrings::readDNAStringSet(
  filepath = file.path(input_dir, "asv.fasta"),
  format = "fasta"
)
```

We will also need some information about the samples, the file is located is another folder.

```{r}
context <- read.table(here::here("data",
                                 "context",
                                 "mapfilecoralineV5.txt"),
                      sep = "\t",
                      header = TRUE,
                      row.names = 1)
```

## Get a phyloseq object

### Check sample file

```{r}
colnames(asv_table) |> sort()
```

```{r}
row.names(context) |> sort()
```

```{r}
setdiff(x = colnames(asv_table),
        y = row.names(context))
```

Perfect! The ASV table sample names match with the contextual data table.

### Assemble ASV table, taxonomy and contextual data

```{r}
physeq <- phyloseq::phyloseq(
  phyloseq::otu_table(asv_table, taxa_are_rows = TRUE),
  phyloseq::tax_table(as.matrix(taxonomy)),
  phyloseq::sample_data(context),
  phyloseq::refseq(asv_seq)
)

saveRDS(physeq,
        file = file.path(input_dir, "phyloseq_object.rds"))
```

```{r}
sessionInfo()
```
